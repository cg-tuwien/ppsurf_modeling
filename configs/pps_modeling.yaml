debug: False
seed_everything: 42

trainer:

  # TODO: Task 3: Try to train the model for different number of epochs. If you change the number of epochs,
  # make sure to also change the learning rate scheduler milestones accordingly. The milestones are the epochs
  # at which the learning rate is reduced by the factor gamma. We provided some examples for different numbers
  # of epochs and the corresponding milestones. You can also try different numbers of epochs and milestones.

  #max_epochs: 10 # sched: 8 [3 min RTX3060]
  max_epochs: 50 # sched: 45,48 [10 min RTX3060]
  #max_epochs: 75 # sched: 55, 70 [20 min RTX3060]
  #max_epochs: 150  # sched: 75, 125 [30 min RTX3060]

  # HINT: If you change the accelerator to 'cpu', you can expect magnitutes of slower training times. To stay sane
  # during this assignment, we recommend to look for a workstation with a GPU! Contact your supervisor if you need
  # help with that.
  accelerator: gpu # cpu
  devices: -1 # Keep at 1 for CPU, -1 for GPU
  precision: 16-mixed # 32-true for CPU, 16-mixed for GPU
  strategy: auto # ddp_spawn for CPU, auto for GPU

  default_root_dir: 'models/pps_modeling' # gets overwritten by pps_modeling.py
  num_sanity_val_steps: 0
  log_every_n_steps: 1
  logger:
    class_path: pytorch_lightning.loggers.TensorBoardLogger
    init_args:
      save_dir: 'models'
      version: 'alpha' # don't change! change name in pps_modeling.py instead
  callbacks:
  - class_path: source.cli.PPSProgressBar
  - class_path: LearningRateMonitor
    init_args:
      logging_interval: step
  - class_path: source.cli.TorchScriptModelCheckpoint
    init_args:
      save_last: True
      save_top_k: 0
      enable_version_counter: False
  - class_path: ModelSummary
    init_args:
      # max_depth: -1
      max_depth: 2

data:
  class_path: source.ppsurf_data_loader.PPSurfDataModule
  init_args:
    use_ddp: False
    in_file: datasets/abc_modeling/testset.txt
    padding_factor: 0.05
    seed: 42
    manifold_points: 10000
    patches_per_shape: -1  # default, all
    do_data_augmentation: True
    # TODO: Task 3: Make sure to adjust the batch size according to the available memory of your GPU.
    # If shared GPU memory is used the training speed will decrease significantly!
    #batch_size: 6 # bei 12GB VRAM [not necessarily speed increase]
    batch_size: 4 # bei 8GB VRAM
    # TODO: Task 3: Make sure you'll adjust the number of workers according to the available CPU cores and memory.
    # The execution will crash if the number of workers is too high for the available memory.
    #workers: 0  # 10 GB RAM single-threaded for debugging
    workers: 8  # default, 16 GB RAM

model:
  class_path: source.ppsurf_model.PPSurfModel
  init_args:
    output_names:
      - 'imp_surf_sign'
    in_channels: 3
    out_channels: 2
    k: 64
    network_latent_size: 256
    num_pts_local: 50
    pointnet_latent_size: 256
    gen_subsample_manifold_iter: 10
    gen_subsample_manifold: 10000
    gen_resolution_global: 129  # half resolution
#    gen_resolution_global: 257
    rec_batch_size: 10000
#    rec_batch_size: 25000  # half memory
#    rec_batch_size: 50000
    gen_refine_iter: 10
    workers: 8
    lambda_l1: 0.0
    results_dir: 'results'
    name: 'ppsurf'
    debug: False

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.001
    betas:
    - 0.9
    - 0.999
    eps: 1e-5
    weight_decay: 1e-2
    amsgrad: False

lr_scheduler:
  class_path: torch.optim.lr_scheduler.MultiStepLR
  init_args:
    # TODO: Task 3: Adapt the milestones according to the number of epochs you chose.
    milestones:
    - 45
    - 48
    gamma: 0.1
